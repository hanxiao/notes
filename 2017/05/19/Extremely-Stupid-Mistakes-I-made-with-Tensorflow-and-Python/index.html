<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Extremely Stupid Mistakes I Made With Tensorflow and Python · Han Xiao Notes</title><meta name="description" content="Extremely Stupid Mistakes I Made With Tensorflow and Python - Han Xiao"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Han Xiao Notes"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://de.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Extremely Stupid Mistakes I Made With Tensorflow and Python</h1><div class="post-info">May 19, 2017</div><div class="post-content"><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Recently I started with Tensorflow for developing some RNN-based system. I choose Python 3 as the main language since TF has most stable API support for it. Plus, I can quickly set up web services via Flask and uWSGI. Previously I had some experience with this technology stack (Python+Flask+uWSGI) in production and I want to make it better this time. Although Java + Spring or Scala + akka may be better options for building a more scalable web app, they are probably overkill in my project, at least for now.</p>
<p>Building a highly scalable and available deep learning system is a topic for another day, here I want to talk about <a id="more"></a>some mistakes I made when using Tensorflow with python. This section is expected to grow continuously as I’m still learning Tensorflow. Some mistakes are extremely embarrassing as they become such obvious once I understand them. </p>
<h2 id="Mistake-1-I-used-dict-get-for-switch-case"><a href="#Mistake-1-I-used-dict-get-for-switch-case" class="headerlink" title="Mistake 1: I used dict.get for switch-case"></a>Mistake 1: I used <code>dict.get</code> for switch-case</h2><p>Python doesn’t have switch-case, which means you have to write a long <code>if-elif-elif-else</code> block to do switching. As I used Scala a lot in my last project, I really miss the powerful and functional <a href="http://docs.scala-lang.org/tutorials/tour/pattern-matching.html" target="_blank" rel="external">pattern matching</a> in Scala.<br>As a workaround, <a href="http://stackoverflow.com/questions/60208/replacements-for-switch-statement-in-python" target="_blank" rel="external">the highest-voted solution on StackOverflow</a> teaches me to use <code>dict.get</code>:<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">'a'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'b'</span>: <span class="number">2</span>,</div><div class="line">    &#125;[x]</div></pre></td></tr></table></figure></p>
<p>Looks legit to me! And to follow this spirit, I wrote something like:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss_node</span><span class="params">(loss_type: str)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">'sigmoid'</span>: tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_logit, logits=d_conf.Y),</div><div class="line">        <span class="string">'weight-sigmoid'</span>: tf.nn.weighted_cross_entropy_with_logits(targets=d_conf.Y, logits=self.Y_logit, pos_weight=tr_conf.pos_weight),</div><div class="line">        <span class="string">'rank-hinge'</span>: tf.map_fn(rank_hinge_loss, tf.stack([self.Y_logit, d_conf.Y], axis=<span class="number">1</span>),</div><div class="line">        <span class="string">'rank-sigmoid'</span>: tf.map_fn(rank_sigmoid_loss, tf.stack([self.Y_logit, d_conf.Y], axis=<span class="number">1</span>)</div><div class="line">        &#125;[loss_type]</div></pre></td></tr></table></figure></p>
<p>I use the Python 3 type annotation feature, that’s why you see <code>str</code> in the function argument. The main idea of this code snippet is to build different types of “loss node” in the Tensorflow computational graph according to the parameter <code>loss_type</code>. Comparing to the lame <code>if-else</code> block, I managed to save some lines. Pretty neat, right? Actually, NO!</p>
<p>The reason is: when you run this code, Python first constructs a <code>dict</code> variable with four items, each of which has a <code>str</code> key and <code>tf.Tensor</code> as value. Then it picks the one that matches the given <code>loss_type</code>. In other words, you create three useless nodes in the computational graph! To show how much time is wasted here, I made a small test.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> timeit</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo1</span><span class="params">(choice: str)</span>:</span></div><div class="line">    <span class="keyword">if</span> choice == <span class="string">'a'</span>:</div><div class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">3</span>)</div><div class="line">    <span class="keyword">elif</span> choice == <span class="string">'b'</span>:</div><div class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">6</span>)</div><div class="line">    <span class="keyword">elif</span> choice == <span class="string">'c'</span>:</div><div class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">9</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo2</span><span class="params">(choice: str)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">'a'</span>: tf.Variable(<span class="number">3</span>),</div><div class="line">        <span class="string">'b'</span>: tf.Variable(<span class="number">6</span>),</div><div class="line">        <span class="string">'c'</span>: tf.Variable(<span class="number">9</span>)</div><div class="line">    &#125;[choice]</div><div class="line"></div><div class="line">node_types = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo1_from_list</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> [foo1(v) <span class="keyword">for</span> v <span class="keyword">in</span> node_types]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo2_from_list</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> [foo2(v) <span class="keyword">for</span> v <span class="keyword">in</span> node_types]</div><div class="line"></div><div class="line">print(<span class="string">'foo1 takes %.3fs'</span> % timeit.timeit(foo1_from_list, number=<span class="number">100</span>))</div><div class="line">tf.reset_default_graph()</div><div class="line">print(<span class="string">'foo2 takes %.3fs'</span> % timeit.timeit(foo2_from_list, number=<span class="number">100</span>))</div></pre></td></tr></table></figure></p>
<p>which gives:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">foo1 takes 1.026s</div><div class="line">foo2 takes 3.091s</div></pre></td></tr></table></figure></p>
<p>Our <code>dict</code> trick is about three times slower than the lame <code>if-else</code>. However, slowness isn’t the only problem, it also significantly complicates the computational graph and induce some error. See the example below, where I create a bidirectional RNN chain:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">fw_cell = rnn.LSTMCell(<span class="number">5</span>)</div><div class="line">bk_cell = rnn.LSTMCell(<span class="number">5</span>)</div><div class="line"></div><div class="line">bi_direction = <span class="keyword">True</span></div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32, [<span class="number">10</span>, <span class="number">5</span>, <span class="number">4</span>], name=<span class="string">'X'</span>)</div><div class="line">L = tf.placeholder(tf.int32, [<span class="number">10</span>], name=<span class="string">'L'</span>)</div><div class="line"></div><div class="line">val, _ = &#123;</div><div class="line">    <span class="keyword">True</span>: tf.nn.bidirectional_dynamic_rnn(fw_cell,</div><div class="line">                                          bk_cell,</div><div class="line">                                          inputs=X,</div><div class="line">                                          sequence_length=L,</div><div class="line">                                          dtype=tf.float32),</div><div class="line">    <span class="keyword">False</span>: tf.nn.dynamic_rnn(fw_cell,</div><div class="line">                             inputs=X,</div><div class="line">                             sequence_length=L,</div><div class="line">                             dtype=tf.float32)</div><div class="line">&#125;[bi_direction]</div></pre></td></tr></table></figure></p>
<p>We can use Tensorboard to visualize the computational graph behind, the <code>dict</code> trick produces:<br><img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-com.png" alt="graph-com.png" title=""><br>whereas the <code>if-else</code> gives a much simpler one:<br><img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-simple.png" alt="graph-simple.png" title=""></p>
<p>On the latest Tensorflow (&gt; 1.1, probably starting from May 2017), the above code will also throw an error: </p>
<p><div class="tip"><br>    ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.basiclstmcell object="" at="" 0x10210d5c0=""> with a different variable scope than its first use. First use of cell was with scope ‘rnn/multi_rnn_cell/cell_0/basic_lstm_cell’, this attempt is with scope ‘rnn/multi_rnn_cell/cell_1/basic_lstm_cell’. Please create a new instance of the cell if you would like it to use a different set of weights. If before you were using: MultiRNNCell([BasicLSTMCell(…)] * num<em>layers), change to: MultiRNNCell([BasicLSTMCell(…) for </em> in range(num_layers)])<br></tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.basiclstmcell></div><br>The reason is that <code>dict</code> creates two RNN chains that share the same RNN cell, i.e. <code>fw_cell</code>. </p>
<h4 id="But-I-still-want-to-be-cool"><a href="#But-I-still-want-to-be-cool" class="headerlink" title="But I still want to be cool"></a>But I still want to be cool</h4><p>How come I didn’t notice this before? Well, probably because I mainly used this trick on some simple task such as printing constants, where the side-effect of the dictionary construction is neglectable. But what if one still want to use this trick instead of writing the <code>if-else</code> block? <code>lambda</code> expression can be a workaround.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo3</span><span class="params">(choice: str)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">'a'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">3</span>),</div><div class="line">        <span class="string">'b'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">6</span>),</div><div class="line">        <span class="string">'c'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">9</span>)</div><div class="line">    &#125;[choice]()</div></pre></td></tr></table></figure></p>
<p>In this code, I wrap the variable with a python lambda expression (i.e. an in-line function), so that the value of a dictionary item is a function handler rather than a TF variable. In fact, TF variable is not constructed until <code>()</code> is called (after <code>[choice]</code>). The speed test also shows that it takes the same time as <code>if-else</code> block.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">foo1 takes 1.132s</div><div class="line">foo3 takes 1.096s</div></pre></td></tr></table></figure></p>
<h2 id="Mistake-2-I-used-lambda-expressions-a-lot"><a href="#Mistake-2-I-used-lambda-expressions-a-lot" class="headerlink" title="Mistake 2: I used lambda expressions a lot"></a>Mistake 2: I used lambda expressions a lot</h2><p>Do I and lambda expression lived happily ever after? Well, until I realized that lambda expression is <strong>the main reason of low GPU utilization</strong>. </p>
<p>The code below employs lambda expression to unify the function arguments of different loss functions in a multi-label classification problem:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">my_loss = &#123;</div><div class="line">    <span class="string">'rank-hinge'</span>: <span class="keyword">lambda</span> y_l, y_t: rank_hinge_loss(y_l, y_t),</div><div class="line">    <span class="string">'rank-sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: rank_sigmoid_loss(y_l, y_t),</div><div class="line">    <span class="string">'weight-sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: tf.nn.weighted_cross_entropy_with_logits(targets=y_t, logits=y_l, pos_weight=<span class="number">3</span>),</div><div class="line">    <span class="string">'sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: tf.nn.sigmoid_cross_entropy_with_logits(labels=y_t, logits=y_l)</div><div class="line">&#125;[loss_type]</div></pre></td></tr></table></figure></p>
<p><code>rank_hinge_loss</code> and <code>rank_sigmoid_loss</code> are customized loss function written by me, which could also be a topic on another day. <code>sigmoid_cross_entropy_with_logits</code> and <code>weighted_cross_entropy_with_logits</code> are Tensorflow built-in loss functions for multi-label problem which somehow have different names for the groundtruth (<code>targets</code> and <code>labels</code>). Anyway, by using lambda expression all those differences should go away, and we can just write something like:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">self.Y_logit = tf.add(tf.matmul(last, weight_o), bias_o, name=<span class="string">'logit'</span>)</div><div class="line">self.loss = my_loss(self.Y_logit, self.Y)</div><div class="line"></div><div class="line">train_step = self.train_conf.optimizer.minimize(self.loss)</div><div class="line"></div><div class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(self.train_conf.num_epoch):</div><div class="line">    sess.run(train_step, feed_dict=&#123;</div><div class="line">                                     self.X: train_data[<span class="number">0</span>],</div><div class="line">                                     self.Y: train_data[<span class="number">1</span>],</div><div class="line">                                     self.L: train_data[<span class="number">2</span>]</div><div class="line">                                 &#125;)</div></pre></td></tr></table></figure></p>
<p>However, when I deploy the code on a GPU instance, I found the GPU utilization is far from 100%. On average it’s only 20% utilization, and training speed is not faster than my 4-core laptop. </p>
<p>I added <code>with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:</code> and looked into the variable placement. Surprisingly, all loss-related TF variables are located on CPU not GPU, even though I force them to via <code>with tf.device(&#39;/gpu:0&#39;):</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Loss_1/tags: (Const): /job:localhost/replica:0/task:0/cpu:0</div><div class="line">Loss/Const_3: (Const): /job:localhost/replica:0/task:0/cpu:0</div><div class="line">Loss/Const_2: (Const): /job:localhost/replica:0/task:0/cpu:0</div><div class="line">Loss/logistic_loss/sub/x: (Const): /job:localhost/replica:0/task:0/cpu:0</div><div class="line">Loss/logistic_loss/add/x: (Const): /job:localhost/replica:0/task:0/cpu:0</div><div class="line">Loss/logistic_loss/mul/x: (Const): /job:localhost/replica:0/task:0/cpu:0</div></pre></td></tr></table></figure></p>
<p>Things did not change when I set <code>loss_type = &#39;sigmoid&#39;</code> and used TF built-in loss. What is the problem?</p>
<p>The problem is by wrapping the loss function with a lambda expression, you make a TF variable / tensor / operator a standard Python function, which can not be efficiently evaluated on GPU with TF. As the loss function is evaluated in every iteration, this implementation is significantly slower than the standard TF loss operator. Once again, slowness is not the complete story. If we look at the computational graph with Tensorboard. The lambda version of <code>sigmoid</code> loss looks like this:</p>
<img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-loss.png" alt="graph-loss.png" title="">
<p>The expression <code>lambda x, y: tf.someloss(x, y)</code> adds three extra nodes for <code>x</code>, <code>y</code> and <code>tf.someloss(x, y)</code> to the graph which are not used at all when computing the gradient. To see it more clearly, one can highlight the input traces of the <code>gradients</code> node.</p>
<img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-loss-trace.png" alt="graph-loss-trace.png" title="">
<p>One can observe that, only <code>logistic_loss_1</code> is connected to the <code>gradients</code> node. The other three nodes are completely useless while training. After I refactored the code by removing unnecessary lambda expressions, the GPU utilization grows to 80%. What about the remaining 20%? Did I give up on that and let it go? Of course not, but it is a story for another day. </p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Having done my last project with Scala, I kind of into the functional programming and sugar-code in Scala. The mistakes above may not happen in the first place if some one follows the Pythonic way of programming. But hey, life is more fun when you do some exploration. The lesson learned here is to keep the code simple and readable. When you refactor the code, always take care of the possible side-effect, especially with the computational graph in Tensorflow. In Python, the code is usually interpreted and executed line by line. In Tensorflow <code>sess.run()</code> is the actual starting point, meaning that there is no actual computation until <code>sess.run()</code> invoked. This subtle difference may cause some troubles especially for beginners. </p>
</div></article></div></main><footer><div class="paginator"></div><div id="disqus_thread"></div><script>var disqus_shortname = 'han-xiao-notes';
var disqus_identifier = '2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/';
var disqus_title = 'Extremely Stupid Mistakes I Made With Tensorflow and Python';
var disqus_url = 'http://yoursite.com/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//han-xiao-notes.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 <a href="http://yoursite.com">Han Xiao</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>